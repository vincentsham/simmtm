{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../img/img_0.PNG\"  width=\"1000\" height=\"240\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook demonstrate an apllication of SimMTM, a simple self-supervised learning framework for time series modeling. Self-supervised learning is a learning paradigm that allows model to learn a good representation from the input data itself. The learned representation will be beneficial to some downstream tasks such as forecasting, classification and outlier detection. \n",
    "\n",
    "Self-supervised learning has a lof of success and achieves state-of-the-art performance in some domains, especially in the image domain. In this demo, we will show a self-supervised learning method, SimMTM, in the time-series domain. SimMTM adopts both masked modeling and contrastive modeling to learn a good representation of the input data. By using the learned representation and finetuning it, we achieve a significant improvement compared to the model without self-supervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shamvinc/ssl_time_series/mvts_transformer/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    os.chdir('src')\n",
    "except:\n",
    "    pass\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import math\n",
    "\n",
    "from datasets.datasplit import split_dataset\n",
    "from datasets.data import data_factory, Normalizer, TSRegressionArchive, CSVRegressionArchive\n",
    "from datasets.datasplit import split_dataset\n",
    "from datasets.dataset import collate_superv\n",
    "from models.ts_transformer import model_factory\n",
    "from models.loss import get_loss_module, contrastive_loss\n",
    "from optimizers import get_optimizer\n",
    "\n",
    "from options import Options\n",
    "from running import setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Masked Modeling\n",
    "\n",
    "Self-supervision via a ‘pretext task’ on input data combined with finetuning on labeled data is widely used for improving model performance in language and computer\n",
    "vision. One of the popular self-supervision tasks on language data is masked modeling. Masking modeling is to mask some of the input entries randomly and predict those masked entries by using unmasked entries. By masked modeling, the model can learn the relationship through different features and different timesteps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/img_1.PNG\"  width=\"900\" height=\"240\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/img_2.PNG\"  width=\"900\" height=\"240\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Choice\n",
    "### Random Masking\n",
    "\n",
    "Random Masking is not a good choice to learn a good representation because the model can simply learn to take the average from the neighbour values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../img/img_3.PNG\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Masking\n",
    "\n",
    "Instead, we choose to use the geometric masking method, which is to mask a sequence of the input data randomly. The length of the sequence is followed by a geometric distribution. In this case, the model requires to recover a masked sequence from other unmasked input data. We suggest the expected length of a masked sequence is a half of the whole time series sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "\n",
    "    Returns:\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimMTM ultilizes both contrastive learning and mask modeling to learn the data representation.\n",
    "## 1 - Contrastive Learning\n",
    "\n",
    "when we mask the input time series data, we create many masked views of the input data. We expect that the distance between two views of the same time series sequence is minimized while maximizing the distance between two different sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../img/img_5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The contrastive loss is the following: (Eq. 8 in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"../img/img_6.PNG\"/><center/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo_contrastive_loss(s, batch_size, tau=0.05):\n",
    "    s = s.squeeze(-1) \n",
    "\n",
    "    B = s.shape[0]\n",
    "    v = s.reshape(B, -1)\n",
    "\n",
    "    norm_v = torch.norm(v, p=2, dim=-1).unsqueeze(-1)\n",
    "    v = v/norm_v\n",
    "    u = torch.transpose(v, 0, 1)\n",
    "\n",
    "    R = torch.matmul(v,u)\n",
    "\n",
    " \n",
    "    R = torch.exp(R/tau) # (batch + mask size) x (batch + mask size)\n",
    "    \n",
    "    # number of masks\n",
    "    M = B//batch_size\n",
    "    mask = torch.eye(batch_size, device=R.device).repeat_interleave(M,dim=0).repeat_interleave(M,dim=1)\n",
    "\n",
    "    denom = R * (torch.ones_like(R) - torch.eye(R.shape[0], device=R.device))\n",
    "\n",
    "    denom = R.sum(-1).unsqueeze(-1)\n",
    "\n",
    "    loss = torch.log(R/denom)\n",
    "    \n",
    "\n",
    "    loss = (loss * (mask - torch.eye(R.shape[0], device=R.device))).sum(1)/(M-1) # except no masked unit\n",
    "    loss = loss.mean(0)\n",
    "    \n",
    "    return -loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Masked Modeling\n",
    "\n",
    "SimMTM proposes to recover a time serie by the weighted sum of multiple masked points, which eases the reconstruction task by assembling ruined but complementary temporal variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../img/img_4.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.ts_transformer import LearnablePositionalEncoding, TransformerBatchNormEncoderLayer\n",
    "\n",
    "class DemoSimMTMTransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len, feat_dim, out_len, out_dim, d_model=16, n_heads=4, num_layers=2, dim_feedforward=32, dropout=0.2, temporal_unit=3):\n",
    "        super(DemoSimMTMTransformerEncoder, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.tau = 0.05\n",
    "        self.mask_length = max_len//2\n",
    "        self.mask_rate = 0.5\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.projector_layer = nn.Linear(max_len, 1)\n",
    "        self.pos_enc1 = LearnablePositionalEncoding(d_model, dropout=dropout, max_len=max_len)\n",
    "        self.pos_enc2 = LearnablePositionalEncoding(d_model, dropout=dropout, max_len=out_len)\n",
    "        \n",
    "        self.act = F.gelu \n",
    "\n",
    "        # encoder_layer = nn.TransformerEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout, activation='gelu')\n",
    "        encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout, activation='gelu')\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout2d(dropout)\n",
    "\n",
    "        # self.predict_layer1 = nn.Conv1d(d_model, 512, 5, stride=1)\n",
    "        self.predict_layer1 = nn.Linear(max_len, out_len)\n",
    "        self.predict_layer2 = nn.Linear(d_model, out_dim)\n",
    "        # self.bn = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.temporal_unit = temporal_unit\n",
    "\n",
    "        self.w1 = torch.nn.parameter.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "        self.w2 = torch.nn.parameter.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Reconstruct the input and create the projected output of X\n",
    "        \n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of original input\n",
    "\n",
    "        Returns:\n",
    "            output: (batch_size, seq_length, feat_dim)\n",
    "            s: (batch_size, d_model, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        _x = X\n",
    "         \n",
    "        # Create masked views of the input X\n",
    "        for i in range(self.temporal_unit):\n",
    "            mask = geom_noise_mask_single(X.shape[0] * X.shape[1] * X.shape[2], self.mask_length, self.mask_rate)\n",
    "            mask = mask.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "            mask = torch.from_numpy(mask).to(X.device)\n",
    "            x_masked = mask * X\n",
    "            _x = torch.cat([_x, x_masked], axis=-1) # [batch_size, seq_length, feat_dim * temporal_unit]\n",
    "    \n",
    "        \n",
    "        _x = _x.reshape(X.shape[0] * (self.temporal_unit + 1), X.shape[1], X.shape[2])\n",
    "  \n",
    "\n",
    "        inp = _x.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * np.sqrt(self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc1(inp)  # add positional encoding\n",
    "\n",
    "        \n",
    "        output = self.transformer_encoder(inp)  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(output)  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        z_hat, _s = self.project(output, self.tau)\n",
    "        # Most probably defining a Linear(d_model,feat_dim) vectorizes the operation over (seq_length, batch_size).\n",
    "        output = self.output_layer(z_hat)  # (batch_size, seq_length, feat_dim)\n",
    "\n",
    "        return output, _s\n",
    "\n",
    "    \n",
    "    def project(self, z, tau):\n",
    "        \"\"\"\n",
    "        Output a weighted average of z\n",
    "        \n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of original input\n",
    "\n",
    "        Returns:\n",
    "            z_hat: (batch_size, seq_length, d_model)\n",
    "            s: (batch_size, d_model, 1)\n",
    "        \"\"\"\n",
    "        _z = z.transpose(1, 2) # [batch_size, d_model, seq_length]\n",
    "        _s = s = self.projector_layer(_z) # [batch_size, d_model, 1]\n",
    "        \n",
    "        if self.training:\n",
    "            mask = torch.ones(1, self.d_model, 1).to(z.device)\n",
    "            mask = self.dropout3(mask)\n",
    "            s = s * mask \n",
    "            s = s + torch.randn(s.shape).to(z.device) * 1e-2\n",
    "        \n",
    "        \n",
    "        s = s.squeeze(-1) \n",
    "        B = s.shape[0]\n",
    "        v = s.reshape(B, -1)\n",
    "\n",
    "        norm_v = torch.norm(v, p=2, dim=-1).unsqueeze(-1)\n",
    "        v = v/norm_v\n",
    "        u = torch.transpose(v, 0, 1)\n",
    "        \n",
    "        R = torch.matmul(v,u)\n",
    "     \n",
    "  \n",
    "        R = torch.exp(R/tau) # (batch + mask size) x (batch + mask size)\n",
    "        R = R * (torch.ones_like(R) - torch.eye(R.shape[0], device=R.device)) # zero out the weight of no masked component\n",
    "        R = R/R.sum(-1).unsqueeze(-1)\n",
    "        M = self.temporal_unit + 1\n",
    "        R = R[::M] # extract every no mask unit # (batch size) x (batch + mask size)\n",
    "\n",
    "        z_hat = (R.unsqueeze(-1).unsqueeze(-1) * z.unsqueeze(0)).sum(1) \n",
    "        return z_hat, _s\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict an output given X\n",
    "        \n",
    "        Args:\n",
    "            z: (batch_size, seq_length, d_model) torch tensor of representations of input\n",
    "            tau: temperture of similarity matrix\n",
    "\n",
    "        Returns:\n",
    "            output: (batch_size, out_seq_len, out_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        inp = X.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * np.sqrt(self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc1(inp)  # add positional encoding\n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "\n",
    "        output = self.transformer_encoder(inp)\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        # output = self.dropout1(output)\n",
    "       \n",
    "        output = output.transpose(1, 2) # (batch_size, d_model, seq_length)\n",
    "        output = self.predict_layer1(output)\n",
    "        # output = self.act(output)\n",
    "        \n",
    "        output = output.transpose(1, 2) # (batch_size, seq_length, d_model)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.pos_enc2(output)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.dropout2(output)\n",
    "        output = self.predict_layer2(output) \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preparation\n",
    "\n",
    "In this demo, we use a benchmask time series dataset called ETT, which contains the time series of oil temperature and power load collected by electricity\n",
    "transformers from July 2016 to July 2018. ETT is a group of four subsets with different recorded frequencies: ETTh1/ETTh2 are recorded every hour, and ETTm1/ETTm2 are recorded every 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 07:27:01,815 | INFO : Stored configuration file in '../experiments/_2023-08-24_07-27-00_hhL'\n"
     ]
    }
   ],
   "source": [
    "args = Options().parse()  \n",
    "args.data_dir = '../datasets/ETTh1'\n",
    "args.task = 'regression'\n",
    "args.output_dir = '../experiments'\n",
    "config = setup(args)\n",
    "from datasets.data import CSVRegressionArchive\n",
    "data = CSVRegressionArchive(config['data_dir'], pattern='TRAIN', config=config)\n",
    "_data = data\n",
    "\n",
    "# Standard Normalization\n",
    "normalizer = Normalizer(config['normalization'])\n",
    "data.feature_df = normalizer.normalize(data.feature_df)\n",
    "data.labels_df = data.feature_df\n",
    "\n",
    "train_slice = slice(None, 12*30*24)\n",
    "val_slice = slice(12*30*24, 16*30*24)\n",
    "test_slice = slice(16*30*24, 20*30*24)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.219043</td>\n",
       "      <td>-0.114203</td>\n",
       "      <td>-0.395671</td>\n",
       "      <td>-0.231896</td>\n",
       "      <td>0.976327</td>\n",
       "      <td>0.805715</td>\n",
       "      <td>2.008455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.238003</td>\n",
       "      <td>-0.081398</td>\n",
       "      <td>-0.411344</td>\n",
       "      <td>-0.251793</td>\n",
       "      <td>0.923944</td>\n",
       "      <td>0.857420</td>\n",
       "      <td>1.688155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.313840</td>\n",
       "      <td>-0.245425</td>\n",
       "      <td>-0.442544</td>\n",
       "      <td>-0.291035</td>\n",
       "      <td>0.610506</td>\n",
       "      <td>0.602230</td>\n",
       "      <td>1.688155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.323320</td>\n",
       "      <td>-0.147009</td>\n",
       "      <td>-0.442544</td>\n",
       "      <td>-0.271138</td>\n",
       "      <td>0.636268</td>\n",
       "      <td>0.703972</td>\n",
       "      <td>1.367970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.285401</td>\n",
       "      <td>-0.147009</td>\n",
       "      <td>-0.411344</td>\n",
       "      <td>-0.231896</td>\n",
       "      <td>0.688651</td>\n",
       "      <td>0.703972</td>\n",
       "      <td>1.006581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17415</th>\n",
       "      <td>-1.280344</td>\n",
       "      <td>0.640322</td>\n",
       "      <td>-1.452361</td>\n",
       "      <td>0.691116</td>\n",
       "      <td>0.348592</td>\n",
       "      <td>1.110943</td>\n",
       "      <td>-0.282559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17416</th>\n",
       "      <td>-1.820544</td>\n",
       "      <td>1.001183</td>\n",
       "      <td>-1.967523</td>\n",
       "      <td>0.769600</td>\n",
       "      <td>0.400975</td>\n",
       "      <td>1.364466</td>\n",
       "      <td>-0.266218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17417</th>\n",
       "      <td>-0.645488</td>\n",
       "      <td>0.771544</td>\n",
       "      <td>-0.749561</td>\n",
       "      <td>0.671772</td>\n",
       "      <td>0.558123</td>\n",
       "      <td>1.110943</td>\n",
       "      <td>-0.356448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17418</th>\n",
       "      <td>0.264279</td>\n",
       "      <td>0.771544</td>\n",
       "      <td>0.171637</td>\n",
       "      <td>0.671772</td>\n",
       "      <td>0.505741</td>\n",
       "      <td>0.959163</td>\n",
       "      <td>-0.413995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17419</th>\n",
       "      <td>0.387515</td>\n",
       "      <td>0.640322</td>\n",
       "      <td>0.275782</td>\n",
       "      <td>0.377182</td>\n",
       "      <td>0.558123</td>\n",
       "      <td>1.009200</td>\n",
       "      <td>-0.438624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17420 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           HUFL      HULL      MUFL      MULL      LUFL      LULL        OT\n",
       "0     -0.219043 -0.114203 -0.395671 -0.231896  0.976327  0.805715  2.008455\n",
       "1     -0.238003 -0.081398 -0.411344 -0.251793  0.923944  0.857420  1.688155\n",
       "2     -0.313840 -0.245425 -0.442544 -0.291035  0.610506  0.602230  1.688155\n",
       "3     -0.323320 -0.147009 -0.442544 -0.271138  0.636268  0.703972  1.367970\n",
       "4     -0.285401 -0.147009 -0.411344 -0.231896  0.688651  0.703972  1.006581\n",
       "...         ...       ...       ...       ...       ...       ...       ...\n",
       "17415 -1.280344  0.640322 -1.452361  0.691116  0.348592  1.110943 -0.282559\n",
       "17416 -1.820544  1.001183 -1.967523  0.769600  0.400975  1.364466 -0.266218\n",
       "17417 -0.645488  0.771544 -0.749561  0.671772  0.558123  1.110943 -0.356448\n",
       "17418  0.264279  0.771544  0.171637  0.671772  0.505741  0.959163 -0.413995\n",
       "17419  0.387515  0.640322  0.275782  0.377182  0.558123  1.009200 -0.438624\n",
       "\n",
       "[17420 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 72\n",
    "out_size = 24\n",
    "out_len = 7\n",
    "# config['data_window_len'] = max_len\n",
    "# config['task'] = 'simmtm'\n",
    "# config['normalization_layer'] = 'BatchNorm'\n",
    "# config['out_len'] = 24\n",
    "# config['out_dim'] = 7\n",
    "# config['d_model'] = 16\n",
    "# config['dim_feedforward'] = 128\n",
    "# config['num_heads'] = 4\n",
    "# config['num_layers'] = 1\n",
    "# from models.ts_transformer import model_factory\n",
    "# model = model_factory(config, data)\n",
    "model = DemoSimMTMTransformerEncoder(max_len=max_len, feat_dim=data.feature_df.shape[1], out_len=out_size, out_dim=out_len, \n",
    "                                     d_model=4, n_heads=4, num_layers=2, dim_feedforward=8)\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "model.tau = 0.05\n",
    "model.mask_length = max_len//2\n",
    "model.mask_ratio = 0.5\n",
    "model.temporal_unit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_indices = data.feature_df[train_slice].index.values[:-max_len-out_size]\n",
    "val_indices = data.feature_df[val_slice].index.values[:-max_len-out_size]\n",
    "test_indices = data.feature_df[test_slice].index.values[:-max_len-out_size]\n",
    "\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_indices, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 3.76 - MSE loss: 0.91 - Contrastive loss: 3.72: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.52it/s]\n",
      "Epoch 1 - Training loss: 4.50 - MSE loss: 1.09 - Contrastive loss: 4.53:   1%|█                                                                       | 2/134 [00:00<00:10, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 6.40 1.07 5.97 - Validation loss: 4.97 1.37 4.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 3.29 - MSE loss: 1.15 - Contrastive loss: 3.45: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.98it/s]\n",
      "Epoch 2 - Training loss: 3.53 - MSE loss: 0.95 - Contrastive loss: 4.07:   1%|█                                                                       | 2/134 [00:00<00:10, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 3.85 0.94 4.20 - Validation loss: 4.27 1.30 4.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 2.78 - MSE loss: 0.71 - Contrastive loss: 3.43: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.95it/s]\n",
      "Epoch 3 - Training loss: 3.19 - MSE loss: 0.92 - Contrastive loss: 3.99:   1%|█                                                                       | 2/134 [00:00<00:10, 12.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 3.32 0.85 4.07 - Validation loss: 3.87 1.27 4.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 2.66 - MSE loss: 0.71 - Contrastive loss: 3.49: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.91it/s]\n",
      "Epoch 4 - Training loss: 2.89 - MSE loss: 0.74 - Contrastive loss: 3.95:   1%|█                                                                       | 2/134 [00:00<00:10, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 3.03 0.77 4.02 - Validation loss: 3.66 1.25 4.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 2.32 - MSE loss: 0.52 - Contrastive loss: 3.24: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.92it/s]\n",
      "Epoch 5 - Training loss: 2.71 - MSE loss: 0.61 - Contrastive loss: 4.04:   1%|█                                                                       | 2/134 [00:00<00:10, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 2.82 0.70 4.00 - Validation loss: 3.55 1.22 5.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 2.39 - MSE loss: 0.59 - Contrastive loss: 3.50: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.94it/s]\n",
      "Epoch 6 - Training loss: 2.57 - MSE loss: 0.53 - Contrastive loss: 4.08:   1%|█                                                                       | 2/134 [00:00<00:10, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 2.65 0.64 3.97 - Validation loss: 3.54 1.21 5.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 2.24 - MSE loss: 0.60 - Contrastive loss: 3.22: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.89it/s]\n",
      "Epoch 7 - Training loss: 2.55 - MSE loss: 0.64 - Contrastive loss: 4.00:   1%|█                                                                       | 2/134 [00:00<00:10, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 2.52 0.59 3.93 - Validation loss: 3.51 1.20 5.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 2.30 - MSE loss: 0.62 - Contrastive loss: 3.47: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.87it/s]\n",
      "Epoch 8 - Training loss: 2.41 - MSE loss: 0.55 - Contrastive loss: 3.98:   1%|█                                                                       | 2/134 [00:00<00:10, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 2.43 0.57 3.91 - Validation loss: 3.47 1.18 5.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 2.26 - MSE loss: 0.63 - Contrastive loss: 3.43: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.74it/s]\n",
      "Epoch 9 - Training loss: 2.30 - MSE loss: 0.50 - Contrastive loss: 3.92:   1%|█                                                                       | 2/134 [00:00<00:10, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 2.37 0.57 3.89 - Validation loss: 3.37 1.17 5.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 2.12 - MSE loss: 0.57 - Contrastive loss: 3.24: 100%|██████████████████████████████████████████████████████████████████████| 134/134 [00:10<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 2.31 0.56 3.86 - Validation loss: 3.35 1.16 5.61\n",
      "Best Epoch 9 - Best Validation loss: 3.348235845565796\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "max_epoch = 10\n",
    "best_loss = 1e10\n",
    "best_epoch = 0\n",
    "device = \"cuda\"\n",
    "loss_fn = nn.MSELoss()\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "while i < max_epoch:\n",
    "    train_loss = { \"loss\": [], \"loss_mse\": [], \"loss_con\": []}\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    \n",
    "    for IDs in progress_bar:\n",
    "        model.train()\n",
    "        X = list(map(lambda idx: np.expand_dims(data.feature_df.loc[idx:idx+max_len-1].to_numpy(), 0), IDs))\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        X = torch.tensor(X).to(device)\n",
    "        X = X.float()\n",
    "        X = X.reshape(X.shape[0], max_len, -1)\n",
    "        # X = X[:, :, -1:]\n",
    "        \n",
    "        pred, s = model(X)  # (batch_size, padded_length, feat_dim)\n",
    "        \n",
    "        loss_mse = loss_fn(pred, X) \n",
    "\n",
    "        loss_con = demo_contrastive_loss(s, X.shape[0])\n",
    "\n",
    "        loss = 1/(model.w1.pow(2)) * loss_mse + 1/(model.w2.pow(2)) * loss_con + torch.log(model.w1) + torch.log(model.w2)\n",
    "  \n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "        optimizer.step()\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        progress_bar.set_description(\"Epoch {0} - Training loss: {1:.2f} - MSE loss: {2:.2f} - Contrastive loss: {3:.2f}\".format(i, \n",
    "                loss.cpu().detach().numpy().item(), loss_mse.cpu().detach().numpy().item(), loss_con.cpu().detach().numpy().item())) \n",
    "        train_loss[\"loss\"].append(loss)\n",
    "        train_loss[\"loss_mse\"].append(loss_mse)\n",
    "        train_loss[\"loss_con\"].append(loss_con)\n",
    "    \n",
    "            \n",
    "    with torch.no_grad():\n",
    "        val_loss = { \"loss\": [], \"loss_mse\": [], \"loss_con\": []}\n",
    "        for IDs in val_dataloader:\n",
    "            model.eval()\n",
    "            X = list(map(lambda idx: np.expand_dims(data.feature_df.loc[idx:idx+max_len-1].to_numpy(), 0), IDs))\n",
    "            X = np.concatenate(X, axis=0)\n",
    "            X = torch.tensor(X).to(device)\n",
    "            X = X.float()\n",
    "            X = X.reshape(X.shape[0], max_len, -1)\n",
    "            # X = X[:, :, -1:]\n",
    "\n",
    "\n",
    "            pred, s = model(X)  # (batch_size, padded_length, feat_dim)\n",
    "        \n",
    "            loss_mse = loss_fn(pred, X) \n",
    "\n",
    "            loss_con = demo_contrastive_loss(s, X.shape[0])\n",
    "\n",
    "            loss = 1/(model.w1.pow(2)) * loss_mse + 1/(model.w2.pow(2)) * loss_con + torch.log(model.w1) + torch.log(model.w2)\n",
    "\n",
    "            val_loss[\"loss\"].append(loss)\n",
    "            val_loss[\"loss_mse\"].append(loss_mse)\n",
    "            val_loss[\"loss_con\"].append(loss_con)\n",
    "\n",
    "        train_loss[\"loss\"] = torch.tensor(train_loss[\"loss\"]).mean()\n",
    "        train_loss[\"loss_mse\"] = torch.tensor(train_loss[\"loss_mse\"]).mean()\n",
    "        train_loss[\"loss_con\"] = torch.tensor(train_loss[\"loss_con\"]).mean()\n",
    "        val_loss[\"loss\"] = torch.tensor(val_loss[\"loss\"]).mean()\n",
    "        val_loss[\"loss_mse\"] = torch.tensor(val_loss[\"loss_mse\"]).mean()\n",
    "        val_loss[\"loss_con\"] = torch.tensor(val_loss[\"loss_con\"]).mean()\n",
    "\n",
    "        if val_loss[\"loss\"] < best_loss:\n",
    "            best_loss = val_loss[\"loss\"]\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = i\n",
    "    \n",
    "        progress_bar.write(\"Epoch {0} - Training loss: {1:.2f} {2:.2f} {3:.2f} - Validation loss: {4:.2f} {5:.2f} {6:.2f}\".format(i, \n",
    "            train_loss[\"loss\"].cpu().detach().numpy().item(), train_loss[\"loss_mse\"].cpu().detach().numpy().item(), train_loss[\"loss_con\"].cpu().detach().numpy().item(),\n",
    "            val_loss[\"loss\"].cpu().detach().numpy().item(), val_loss[\"loss_mse\"].cpu().detach().numpy().item(), val_loss[\"loss_con\"].cpu().detach().numpy().item()))\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "tqdm.write(\"Best Epoch {} - Best Validation loss: {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Training Loop\n",
    "\n",
    "Our downstream task is to forecast the next 24 hours time series data given the past 72 hours time series data. To do this, we append a linear layer on our learned representation, and finetune it for our downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_model = copy.deepcopy(best_model)\n",
    "optimizer = torch.optim.AdamW(finetune_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.73: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 29.01it/s]\n",
      "Epoch 1 - Training loss: 0.69:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 28.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.82 - Validation loss: 1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.62: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 28.76it/s]\n",
      "Epoch 2 - Training loss: 0.57:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 28.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.64 - Validation loss: 1.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.56: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 29.05it/s]\n",
      "Epoch 3 - Training loss: 0.60:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 29.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 0.59 - Validation loss: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.65: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 29.62it/s]\n",
      "Epoch 4 - Training loss: 0.54:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 29.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 0.56 - Validation loss: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.57: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 27.89it/s]\n",
      "Epoch 5 - Training loss: 0.53:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 28.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 0.53 - Validation loss: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 0.53: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 28.90it/s]\n",
      "Epoch 6 - Training loss: 0.51:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 28.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 0.52 - Validation loss: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 0.49: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 28.26it/s]\n",
      "Epoch 7 - Training loss: 0.47:   2%|██▌                                                                                                               | 3/134 [00:00<00:05, 25.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 0.51 - Validation loss: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 0.51: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:05<00:00, 25.65it/s]\n",
      "Epoch 8 - Training loss: 0.49:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 29.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 0.50 - Validation loss: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 0.55: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 29.35it/s]\n",
      "Epoch 9 - Training loss: 0.48:   2%|██▌                                                                                                               | 3/134 [00:00<00:04, 28.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 0.50 - Validation loss: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 0.53: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 134/134 [00:04<00:00, 27.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 0.50 - Validation loss: 0.72\n",
      "Best Epoch 9 - Best Validation loss: 0.7233729362487793\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "max_epoch = 10\n",
    "best_loss = 1e10\n",
    "best_finetune_model = copy.deepcopy(best_model)\n",
    "best_epoch = 0\n",
    "device = \"cuda\"\n",
    "finetune_model.to(device)\n",
    "while i < max_epoch:\n",
    "    train_loss = []\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    \n",
    "    for IDs in progress_bar:\n",
    "        finetune_model.train()\n",
    "        \n",
    "        X = list(map(lambda idx: np.expand_dims(data.feature_df.loc[idx:idx+max_len-1].to_numpy(), 0), IDs))\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        X = torch.tensor(X).to(device)\n",
    "        X = X.float()\n",
    "        X = X.reshape(X.shape[0], max_len, -1)\n",
    "        # X = X[:, :, -1:]\n",
    "        \n",
    "        targets =  list(map(lambda idx: np.expand_dims(data.labels_df.loc[idx+max_len:idx+max_len+out_size-1].to_numpy(), 0), IDs))\n",
    "        targets = np.concatenate(targets, axis=0)\n",
    "        # targets = torch.tensor(targets[:,:,-1]).to(device)\n",
    "        targets = torch.tensor(targets).to(device)\n",
    "        targets = targets.float()\n",
    "        targets = targets.reshape(X.shape[0], out_size, -1)\n",
    "        # targets = targets[:, :, -1:]\n",
    "\n",
    "        pred = finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(finetune_model.parameters(), max_norm=4.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_description(\"Epoch {} - Training loss: {:.2f}\".format(i, loss)) \n",
    "        train_loss.append(loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for IDs in val_dataloader:\n",
    "            finetune_model.eval()\n",
    "            X = list(map(lambda idx: np.expand_dims(data.feature_df.loc[idx:idx+max_len-1].to_numpy(), 0), IDs))\n",
    "            X = np.concatenate(X, axis=0)\n",
    "            X = torch.tensor(X).to(device)\n",
    "            X = X.float()\n",
    "            X = X.reshape(X.shape[0], max_len, -1)\n",
    "            # X = X[:, :, -1:]\n",
    "            \n",
    "            targets =  list(map(lambda idx: np.expand_dims(data.labels_df.loc[idx+max_len:idx+max_len+out_size-1].to_numpy(), 0), IDs))\n",
    "            targets = np.concatenate(targets, axis=0)\n",
    "            # targets = torch.tensor(targets[:,:,-1]).to(device)\n",
    "            targets = torch.tensor(targets).to(device)\n",
    "            targets = targets.float()\n",
    "            targets = targets.reshape(X.shape[0], out_size, -1)\n",
    "            # targets = targets[:, :, -1:]\n",
    "\n",
    "            pred = finetune_model.predict(X.float())\n",
    "            pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "            \n",
    "            loss = loss_fn(pred, targets)\n",
    "            val_loss.append(loss)\n",
    "\n",
    "        train_loss = torch.tensor(train_loss).mean()\n",
    "        val_loss = torch.tensor(val_loss).mean()\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_finetune_model = copy.deepcopy(finetune_model)\n",
    "            best_epoch = i\n",
    "    \n",
    "    progress_bar.write(\"Epoch {} - Training loss: {:.2f} - Validation loss: {:.2f}\".format(i, train_loss, val_loss))\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "tqdm.write(\"Best Epoch {} - Best Validation loss: {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5565699934959412\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "with torch.no_grad():\n",
    "    for IDs in test_dataloader:\n",
    "        best_finetune_model.eval()\n",
    "        X = list(map(lambda idx: np.expand_dims(data.feature_df.loc[idx:idx+max_len-1].to_numpy(), 0), IDs))\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        X = torch.tensor(X).to(device)\n",
    "        X = X.float()\n",
    "        X = X.reshape(X.shape[0], max_len, -1)\n",
    "        # X = X[:, :, -1:]\n",
    "        \n",
    "\n",
    "        targets =  list(map(lambda idx: np.expand_dims(data.labels_df.loc[idx+max_len:idx+max_len+out_size-1].to_numpy(), 0), IDs))\n",
    "        targets = np.concatenate(targets, axis=0)\n",
    "        # targets = torch.tensor(targets[:,:,-1]).to(device)\n",
    "        targets = torch.tensor(targets).to(device)\n",
    "        targets = targets.float()\n",
    "        targets = targets.reshape(X.shape[0], out_size, -1)\n",
    "        # targets = targets[:, :, -1:]\n",
    "        \n",
    "\n",
    "        pred = best_finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "\n",
    "        test_loss.append(loss)\n",
    "\n",
    "\n",
    "test_loss = torch.tensor(test_loss).mean()\n",
    "print(\"Test MSE loss: {}\".format(test_loss))\n",
    "print(\"Test RMSE loss: {}\".format(np.sqrt(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://arxiv.org/abs/2302.00861\n",
    "2. https://github.com/gzerveas/mvts_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
