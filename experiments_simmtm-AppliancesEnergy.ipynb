{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../img/img_0.PNG\"  width=\"1000\" height=\"240\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook demonstrate an apllication of SimMTM, a simple self-supervised learning framework for time series modeling. Self-supervised learning is a learning paradigm that allows model to learn a good representation from the input data itself. The learned representation will be beneficial to some downstream tasks such as forecasting, classification and outlier detection. \n",
    "\n",
    "Self-supervised learning has a lof of success and achieves state-of-the-art performance in some domains, especially in the image domain. In this demo, we will show a self-supervised learning method, SimMTM, in the time-series domain. SimMTM adopts both masked modeling and contrastive modeling to learn a good representation of the input data. By using the learned representation and finetuning it, we achieve a significant improvement compared to the model without self-supervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shamvinc/ssl_time_series/mvts_transformer/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    os.chdir('src')\n",
    "except:\n",
    "    pass\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import math\n",
    "\n",
    "from datasets.datasplit import split_dataset\n",
    "from datasets.data import data_factory, Normalizer, TSRegressionArchive, CSVRegressionArchive\n",
    "from datasets.datasplit import split_dataset\n",
    "from datasets.dataset import collate_superv\n",
    "from models.ts_transformer import model_factory\n",
    "from models.loss import get_loss_module, contrastive_loss\n",
    "from optimizers import get_optimizer\n",
    "\n",
    "from options import Options\n",
    "from running import setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Masked Modeling\n",
    "\n",
    "Self-supervision via a ‘pretext task’ on input data combined with finetuning on labeled data is widely used for improving model performance in language and computer\n",
    "vision. One of the popular self-supervision tasks on language data is masked modeling. Masking modeling is to mask some of the input entries randomly and predict those masked entries by using unmasked entries. By masked modeling, the model can learn the relationship through different features and different timesteps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/img_1.PNG\"  width=\"900\" height=\"240\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/img_2.PNG\"  width=\"900\" height=\"240\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Choice\n",
    "### Random Masking\n",
    "\n",
    "Random Masking is not a good choice to learn a good representation because the model can simply learn to take the average from the neighbour values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../img/img_3.PNG\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Masking\n",
    "\n",
    "Instead, we choose to use the geometric masking method, which is to mask a sequence of the input data randomly. The length of the sequence is followed by a geometric distribution. In this case, the model requires to recover a masked sequence from other unmasked input data. We suggest the expected length of a masked sequence is a half of the whole time series sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "\n",
    "    Returns:\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimMTM ultilizes both contrastive learning and mask modeling to learn the data representation.\n",
    "## 1 - Contrastive Learning\n",
    "\n",
    "when we mask the input time series data, we create many masked views of the input data. We expect that the distance between two views of the same time series sequence is minimized while maximizing the distance between two different sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../img/img_5.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The contrastive loss is the following: (Eq. 8 in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"../img/img_6.PNG\"/><center/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo_contrastive_loss(s, batch_size, tau=0.05):\n",
    "    s = s.squeeze(-1) \n",
    "\n",
    "    B = s.shape[0]\n",
    "    v = s.reshape(B, -1)\n",
    "\n",
    "    norm_v = torch.norm(v, p=2, dim=-1).unsqueeze(-1)\n",
    "    v = v/norm_v\n",
    "    u = torch.transpose(v, 0, 1)\n",
    "\n",
    "    R = torch.matmul(v,u)\n",
    "\n",
    " \n",
    "    R = torch.exp(R/tau) # (batch + mask size) x (batch + mask size)\n",
    "    \n",
    "    # number of masks\n",
    "    M = B//batch_size\n",
    "    mask = torch.eye(batch_size, device=R.device).repeat_interleave(M,dim=0).repeat_interleave(M,dim=1)\n",
    "\n",
    "    denom = R * (torch.ones_like(R) - torch.eye(R.shape[0], device=R.device))\n",
    "\n",
    "    denom = R.sum(-1).unsqueeze(-1)\n",
    "\n",
    "    loss = torch.log(R/denom)\n",
    "    \n",
    "\n",
    "    loss = (loss * (mask - torch.eye(R.shape[0], device=R.device))).sum(1)/(M-1) # except no masked unit\n",
    "    loss = loss.mean(0)\n",
    "    \n",
    "    return -loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Masked Modeling\n",
    "\n",
    "SimMTM proposes to recover a time serie by the weighted sum of multiple masked points, which eases the reconstruction task by assembling ruined but complementary temporal variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../img/img_4.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.ts_transformer import LearnablePositionalEncoding, TransformerBatchNormEncoderLayer\n",
    "\n",
    "class DemoSimMTMTransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len, feat_dim, out_len, out_dim, d_model=16, n_heads=4, num_layers=2, dim_feedforward=32, dropout=0.2, temporal_unit=3):\n",
    "        super(DemoSimMTMTransformerEncoder, self).__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.tau = 0.05\n",
    "        self.mask_length = max_len//2\n",
    "        self.mask_rate = 0.5\n",
    "\n",
    "        self.project_inp = nn.Linear(feat_dim, d_model)\n",
    "        self.projector_layer = nn.Linear(max_len, 1)\n",
    "        self.pos_enc1 = LearnablePositionalEncoding(d_model, dropout=dropout, max_len=max_len)\n",
    "        self.pos_enc2 = LearnablePositionalEncoding(d_model, dropout=dropout, max_len=out_len)\n",
    "        \n",
    "        self.act = F.gelu \n",
    "\n",
    "        # encoder_layer = nn.TransformerEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout, activation='gelu')\n",
    "        encoder_layer = TransformerBatchNormEncoderLayer(d_model, self.n_heads, dim_feedforward, dropout, activation='gelu')\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, feat_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout2d(dropout)\n",
    "\n",
    "        # self.predict_layer1 = nn.Conv1d(d_model, 512, 5, stride=1)\n",
    "        self.predict_layer1 = nn.Linear(max_len, out_len)\n",
    "        self.predict_layer2 = nn.Linear(d_model, out_dim)\n",
    "        # self.bn = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.temporal_unit = temporal_unit\n",
    "\n",
    "        self.w1 = torch.nn.parameter.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "        self.w2 = torch.nn.parameter.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Reconstruct the input and create the projected output of X\n",
    "        \n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of original input\n",
    "\n",
    "        Returns:\n",
    "            output: (batch_size, seq_length, feat_dim)\n",
    "            s: (batch_size, d_model, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        _x = X\n",
    "         \n",
    "        # Create masked views of the input X\n",
    "        for i in range(self.temporal_unit):\n",
    "            mask = geom_noise_mask_single(X.shape[0] * X.shape[1] * X.shape[2], self.mask_length, self.mask_rate)\n",
    "            mask = mask.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "            mask = torch.from_numpy(mask).to(X.device)\n",
    "            x_masked = mask * X\n",
    "            _x = torch.cat([_x, x_masked], axis=-1) # [batch_size, seq_length, feat_dim * temporal_unit]\n",
    "    \n",
    "        \n",
    "        _x = _x.reshape(X.shape[0] * (self.temporal_unit + 1), X.shape[1], X.shape[2])\n",
    "  \n",
    "\n",
    "        inp = _x.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * np.sqrt(self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc1(inp)  # add positional encoding\n",
    "\n",
    "        \n",
    "        output = self.transformer_encoder(inp)  # (seq_length, batch_size, d_model)\n",
    "        output = self.act(output)  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        output = self.dropout1(output)\n",
    "\n",
    "        z_hat, _s = self.project(output, self.tau)\n",
    "        # Most probably defining a Linear(d_model,feat_dim) vectorizes the operation over (seq_length, batch_size).\n",
    "        output = self.output_layer(z_hat)  # (batch_size, seq_length, feat_dim)\n",
    "\n",
    "        return output, _s\n",
    "\n",
    "    \n",
    "    def project(self, z, tau):\n",
    "        \"\"\"\n",
    "        Output a weighted average of z\n",
    "        \n",
    "        Args:\n",
    "            X: (batch_size, seq_length, feat_dim) torch tensor of original input\n",
    "\n",
    "        Returns:\n",
    "            z_hat: (batch_size, seq_length, d_model)\n",
    "            s: (batch_size, d_model, 1)\n",
    "        \"\"\"\n",
    "        _z = z.transpose(1, 2) # [batch_size, d_model, seq_length]\n",
    "        _s = s = self.projector_layer(_z) # [batch_size, d_model, 1]\n",
    "        \n",
    "        if self.training:\n",
    "            mask = torch.ones(1, self.d_model, 1).to(z.device)\n",
    "            mask = self.dropout3(mask)\n",
    "            s = s * mask \n",
    "            s = s + torch.randn(s.shape).to(z.device) * 1e-2\n",
    "        \n",
    "        \n",
    "        s = s.squeeze(-1) \n",
    "        B = s.shape[0]\n",
    "        v = s.reshape(B, -1)\n",
    "\n",
    "        norm_v = torch.norm(v, p=2, dim=-1).unsqueeze(-1)\n",
    "        v = v/norm_v\n",
    "        u = torch.transpose(v, 0, 1)\n",
    "        \n",
    "        R = torch.matmul(v,u)\n",
    "     \n",
    "  \n",
    "        R = torch.exp(R/tau) # (batch + mask size) x (batch + mask size)\n",
    "        R = R * (torch.ones_like(R) - torch.eye(R.shape[0], device=R.device)) # zero out the weight of no masked component\n",
    "        R = R/R.sum(-1).unsqueeze(-1)\n",
    "        M = self.temporal_unit + 1\n",
    "        R = R[::M] # extract every no mask unit # (batch size) x (batch + mask size)\n",
    "\n",
    "        z_hat = (R.unsqueeze(-1).unsqueeze(-1) * z.unsqueeze(0)).sum(1) \n",
    "        return z_hat, _s\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict an output given X\n",
    "        \n",
    "        Args:\n",
    "            z: (batch_size, seq_length, d_model) torch tensor of representations of input\n",
    "            tau: temperture of similarity matrix\n",
    "\n",
    "        Returns:\n",
    "            output: (batch_size, out_seq_len, out_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n",
    "        inp = X.permute(1, 0, 2)\n",
    "        inp = self.project_inp(inp) * np.sqrt(self.d_model)  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n",
    "        inp = self.pos_enc1(inp)  # add positional encoding\n",
    "        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n",
    "\n",
    "        output = self.transformer_encoder(inp)\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n",
    "        # output = self.dropout1(output)\n",
    "       \n",
    "        output = output.transpose(1, 2) # (batch_size, d_model, seq_length)\n",
    "        output = self.predict_layer1(output)\n",
    "        # output = self.act(output)\n",
    "        \n",
    "        output = output.transpose(1, 2) # (batch_size, seq_length, d_model)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.pos_enc2(output)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.dropout2(output)\n",
    "        output = self.predict_layer2(output) \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preparation\n",
    "\n",
    "In this demo, we use a benchmask time series dataset called AppliancesEnergy.\n",
    "\n",
    "This dataset is part of the Monash, UEA & UCR time series regression repository. http://tseregression.org/\n",
    "\n",
    "The goal of this dataset is to predict total energy usage in kWh of a house. This dataset contains 138 time series obtained from the Appliances Energy Prediction dataset from the UCI repository. The time series has 24 dimensions. This includes temperature and humidity measurements of 9 rooms in a house, monitored with a ZigBee wireless sensor network. It also includes weather and climate data such as temperature, pressure, humidity, wind speed, visibility and dewpoint measured from Chievres airport. The data set is averaged for 10 minutes period and spanning 4.5 months.\n",
    "\n",
    "Please refer to https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction  for more details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 07:59:40,122 | INFO : Stored configuration file in '../experiments/_2023-08-24_07-59-39_pBT'\n",
      "119it [00:04, 29.64it/s]\n",
      "66it [00:01, 38.10it/s] \n"
     ]
    }
   ],
   "source": [
    "args = Options().parse()  \n",
    "args.data_dir = '../datasets/AppliancesEnergy'\n",
    "args.task = 'regression'\n",
    "args.output_dir = '../experiments'\n",
    "config = setup(args)\n",
    "from datasets.data import CSVRegressionArchive\n",
    "data = TSRegressionArchive(config['data_dir'], pattern='TRAIN', config=config)\n",
    "test_data = TSRegressionArchive(config['data_dir'], pattern='TEST', config=config)\n",
    "_data = data\n",
    "\n",
    "# Standard Normalization\n",
    "normalizer = Normalizer(config['normalization'])\n",
    "data.feature_df = normalizer.normalize(data.feature_df)\n",
    "test_data.feature_df = normalizer.normalize(test_data.feature_df)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_14</th>\n",
       "      <th>dim_15</th>\n",
       "      <th>dim_16</th>\n",
       "      <th>dim_17</th>\n",
       "      <th>dim_18</th>\n",
       "      <th>dim_19</th>\n",
       "      <th>dim_20</th>\n",
       "      <th>dim_21</th>\n",
       "      <th>dim_22</th>\n",
       "      <th>dim_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.269570</td>\n",
       "      <td>-2.153832</td>\n",
       "      <td>-0.259319</td>\n",
       "      <td>-2.121628</td>\n",
       "      <td>-0.999057</td>\n",
       "      <td>-1.514857</td>\n",
       "      <td>-0.514935</td>\n",
       "      <td>-1.837894</td>\n",
       "      <td>-1.126190</td>\n",
       "      <td>-0.656912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267595</td>\n",
       "      <td>-1.654712</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.464965</td>\n",
       "      <td>-0.418045</td>\n",
       "      <td>0.330620</td>\n",
       "      <td>-1.948513</td>\n",
       "      <td>1.202557</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-1.981361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.269570</td>\n",
       "      <td>-2.089203</td>\n",
       "      <td>-0.288860</td>\n",
       "      <td>-2.089363</td>\n",
       "      <td>-0.999057</td>\n",
       "      <td>-1.497081</td>\n",
       "      <td>-0.514935</td>\n",
       "      <td>-1.823090</td>\n",
       "      <td>-1.126190</td>\n",
       "      <td>-0.672042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234318</td>\n",
       "      <td>-1.654712</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.456310</td>\n",
       "      <td>-0.436693</td>\n",
       "      <td>0.342184</td>\n",
       "      <td>-1.915849</td>\n",
       "      <td>1.134967</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-1.973379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.269570</td>\n",
       "      <td>-2.056059</td>\n",
       "      <td>-0.303630</td>\n",
       "      <td>-2.027255</td>\n",
       "      <td>-0.999057</td>\n",
       "      <td>-1.464492</td>\n",
       "      <td>-0.528758</td>\n",
       "      <td>-1.786821</td>\n",
       "      <td>-1.088955</td>\n",
       "      <td>-0.668638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199456</td>\n",
       "      <td>-1.640794</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.456310</td>\n",
       "      <td>-0.455342</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>-1.883185</td>\n",
       "      <td>1.067377</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-1.965397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.287474</td>\n",
       "      <td>-2.006345</td>\n",
       "      <td>-0.330216</td>\n",
       "      <td>-1.962726</td>\n",
       "      <td>-0.999057</td>\n",
       "      <td>-1.464492</td>\n",
       "      <td>-0.528758</td>\n",
       "      <td>-1.758695</td>\n",
       "      <td>-1.126190</td>\n",
       "      <td>-0.675447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151917</td>\n",
       "      <td>-1.616753</td>\n",
       "      <td>-0.472082</td>\n",
       "      <td>-0.464965</td>\n",
       "      <td>-0.473990</td>\n",
       "      <td>0.365312</td>\n",
       "      <td>-1.850521</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-1.957415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.323280</td>\n",
       "      <td>-1.988116</td>\n",
       "      <td>-0.343509</td>\n",
       "      <td>-1.907877</td>\n",
       "      <td>-0.999057</td>\n",
       "      <td>-1.464492</td>\n",
       "      <td>-0.544116</td>\n",
       "      <td>-1.726867</td>\n",
       "      <td>-1.126190</td>\n",
       "      <td>-0.686795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123394</td>\n",
       "      <td>-1.628141</td>\n",
       "      <td>-0.472082</td>\n",
       "      <td>-0.464965</td>\n",
       "      <td>-0.492638</td>\n",
       "      <td>0.376875</td>\n",
       "      <td>-1.817857</td>\n",
       "      <td>0.932197</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-1.949433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.104462</td>\n",
       "      <td>-1.923486</td>\n",
       "      <td>-0.166268</td>\n",
       "      <td>-1.946594</td>\n",
       "      <td>-0.907768</td>\n",
       "      <td>-1.286734</td>\n",
       "      <td>-0.408963</td>\n",
       "      <td>-1.883045</td>\n",
       "      <td>-1.126190</td>\n",
       "      <td>-0.545512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437149</td>\n",
       "      <td>-1.565509</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.503523</td>\n",
       "      <td>-0.371424</td>\n",
       "      <td>0.307492</td>\n",
       "      <td>-2.111834</td>\n",
       "      <td>1.540506</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-2.101089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.144247</td>\n",
       "      <td>-2.054402</td>\n",
       "      <td>-0.181038</td>\n",
       "      <td>-2.034514</td>\n",
       "      <td>-0.872534</td>\n",
       "      <td>-1.464492</td>\n",
       "      <td>-0.456574</td>\n",
       "      <td>-1.853438</td>\n",
       "      <td>-1.126190</td>\n",
       "      <td>-0.577475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405457</td>\n",
       "      <td>-1.589549</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.489359</td>\n",
       "      <td>-0.380749</td>\n",
       "      <td>0.312118</td>\n",
       "      <td>-2.079170</td>\n",
       "      <td>1.472917</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-2.077144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.166129</td>\n",
       "      <td>-2.130632</td>\n",
       "      <td>-0.210578</td>\n",
       "      <td>-2.113562</td>\n",
       "      <td>-0.939800</td>\n",
       "      <td>-1.545471</td>\n",
       "      <td>-0.487290</td>\n",
       "      <td>-1.899329</td>\n",
       "      <td>-1.088955</td>\n",
       "      <td>-0.592606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375349</td>\n",
       "      <td>-1.602835</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.482277</td>\n",
       "      <td>-0.390073</td>\n",
       "      <td>0.316744</td>\n",
       "      <td>-2.046506</td>\n",
       "      <td>1.405327</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-2.053198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.209893</td>\n",
       "      <td>-2.213490</td>\n",
       "      <td>-0.259319</td>\n",
       "      <td>-2.146632</td>\n",
       "      <td>-0.970229</td>\n",
       "      <td>-1.556334</td>\n",
       "      <td>-0.487290</td>\n",
       "      <td>-1.862320</td>\n",
       "      <td>-1.107573</td>\n",
       "      <td>-0.618706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346826</td>\n",
       "      <td>-1.616753</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.482277</td>\n",
       "      <td>-0.399397</td>\n",
       "      <td>0.321369</td>\n",
       "      <td>-2.013842</td>\n",
       "      <td>1.337737</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-2.029252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.229785</td>\n",
       "      <td>-2.186975</td>\n",
       "      <td>-0.259319</td>\n",
       "      <td>-2.164378</td>\n",
       "      <td>-0.999057</td>\n",
       "      <td>-1.534608</td>\n",
       "      <td>-0.487290</td>\n",
       "      <td>-1.862320</td>\n",
       "      <td>-1.107573</td>\n",
       "      <td>-0.641403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299287</td>\n",
       "      <td>-1.654712</td>\n",
       "      <td>-0.454857</td>\n",
       "      <td>-0.482277</td>\n",
       "      <td>-0.408721</td>\n",
       "      <td>0.325995</td>\n",
       "      <td>-1.981178</td>\n",
       "      <td>1.270147</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>-2.005306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13680 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim_0     dim_1     dim_2     dim_3     dim_4     dim_5     dim_6  \\\n",
       "0  -0.269570 -2.153832 -0.259319 -2.121628 -0.999057 -1.514857 -0.514935   \n",
       "0  -0.269570 -2.089203 -0.288860 -2.089363 -0.999057 -1.497081 -0.514935   \n",
       "0  -0.269570 -2.056059 -0.303630 -2.027255 -0.999057 -1.464492 -0.528758   \n",
       "0  -0.287474 -2.006345 -0.330216 -1.962726 -0.999057 -1.464492 -0.528758   \n",
       "0  -0.323280 -1.988116 -0.343509 -1.907877 -0.999057 -1.464492 -0.544116   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "94 -0.104462 -1.923486 -0.166268 -1.946594 -0.907768 -1.286734 -0.408963   \n",
       "94 -0.144247 -2.054402 -0.181038 -2.034514 -0.872534 -1.464492 -0.456574   \n",
       "94 -0.166129 -2.130632 -0.210578 -2.113562 -0.939800 -1.545471 -0.487290   \n",
       "94 -0.209893 -2.213490 -0.259319 -2.146632 -0.970229 -1.556334 -0.487290   \n",
       "94 -0.229785 -2.186975 -0.259319 -2.164378 -0.999057 -1.534608 -0.487290   \n",
       "\n",
       "       dim_7     dim_8     dim_9  ...    dim_14    dim_15    dim_16    dim_17  \\\n",
       "0  -1.837894 -1.126190 -0.656912  ...  0.267595 -1.654712 -0.454857 -0.464965   \n",
       "0  -1.823090 -1.126190 -0.672042  ...  0.234318 -1.654712 -0.454857 -0.456310   \n",
       "0  -1.786821 -1.088955 -0.668638  ...  0.199456 -1.640794 -0.454857 -0.456310   \n",
       "0  -1.758695 -1.126190 -0.675447  ...  0.151917 -1.616753 -0.472082 -0.464965   \n",
       "0  -1.726867 -1.126190 -0.686795  ...  0.123394 -1.628141 -0.472082 -0.464965   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "94 -1.883045 -1.126190 -0.545512  ...  0.437149 -1.565509 -0.454857 -0.503523   \n",
       "94 -1.853438 -1.126190 -0.577475  ...  0.405457 -1.589549 -0.454857 -0.489359   \n",
       "94 -1.899329 -1.088955 -0.592606  ...  0.375349 -1.602835 -0.454857 -0.482277   \n",
       "94 -1.862320 -1.107573 -0.618706  ...  0.346826 -1.616753 -0.454857 -0.482277   \n",
       "94 -1.862320 -1.107573 -0.641403  ...  0.299287 -1.654712 -0.454857 -0.482277   \n",
       "\n",
       "      dim_18    dim_19    dim_20    dim_21    dim_22    dim_23  \n",
       "0  -0.418045  0.330620 -1.948513  1.202557  0.106478 -1.981361  \n",
       "0  -0.436693  0.342184 -1.915849  1.134967  0.106478 -1.973379  \n",
       "0  -0.455342  0.353748 -1.883185  1.067377  0.106478 -1.965397  \n",
       "0  -0.473990  0.365312 -1.850521  0.999787  0.106478 -1.957415  \n",
       "0  -0.492638  0.376875 -1.817857  0.932197  0.106478 -1.949433  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "94 -0.371424  0.307492 -2.111834  1.540506  0.106478 -2.101089  \n",
       "94 -0.380749  0.312118 -2.079170  1.472917  0.106478 -2.077144  \n",
       "94 -0.390073  0.316744 -2.046506  1.405327  0.106478 -2.053198  \n",
       "94 -0.399397  0.321369 -2.013842  1.337737  0.106478 -2.029252  \n",
       "94 -0.408721  0.325995 -1.981178  1.270147  0.106478 -2.005306  \n",
       "\n",
       "[13680 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = data.feature_df.loc[0].shape[0]\n",
    "out_size = 1\n",
    "out_dim = 1\n",
    "# config['data_window_len'] = max_len\n",
    "# config['task'] = 'simmtm'\n",
    "# config['normalization_layer'] = 'BatchNorm'\n",
    "# config['out_len'] = 24\n",
    "# config['out_dim'] = 7\n",
    "# config['d_model'] = 16\n",
    "# config['dim_feedforward'] = 128\n",
    "# config['num_heads'] = 4\n",
    "# config['num_layers'] = 1\n",
    "# from models.ts_transformer import model_factory\n",
    "# model = model_factory(config, data)\n",
    "model = DemoSimMTMTransformerEncoder(max_len=max_len, feat_dim=data.feature_df.shape[1], out_len=out_size, out_dim=out_dim, \n",
    "                                     d_model=8, n_heads=4, num_layers=2, dim_feedforward=16)\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "model.tau = 0.05\n",
    "model.mask_length = max_len//2\n",
    "model.mask_ratio = 0.5\n",
    "model.temporal_unit = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "train_indices, val_indices, _ = split_dataset(data_indices=data.all_IDs,\n",
    "                                                         validation_method='ShuffleSplit',\n",
    "                                                         n_splits=1,\n",
    "                                                         validation_ratio=0.2,\n",
    "                                                         test_set_ratio=0,  # used only if test_indices not explicitly specified\n",
    "                                                         test_indices=None,\n",
    "                                                         random_seed=1337,\n",
    "                                                         labels=None)\n",
    "train_indices = train_indices[0]\n",
    "val_indices = val_indices[0]\n",
    "test_indices = np.array(test_data.all_IDs)\n",
    "\n",
    "train_dataloader = DataLoader(train_indices, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_indices, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_indices, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 19.16 - MSE loss: 1.42 - Contrastive loss: 17.78: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.22it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 19.60 1.24 18.38 - Validation loss: 16.28 1.16 15.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 18.82 - MSE loss: 0.81 - Contrastive loss: 18.11: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.30it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 18.75 1.01 17.83 - Validation loss: 11.87 1.12 10.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 13.22 - MSE loss: 1.26 - Contrastive loss: 12.08: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.32it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 15.33 1.18 14.27 - Validation loss: 9.63 1.11 8.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 12.89 - MSE loss: 1.31 - Contrastive loss: 11.73: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.35it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 13.13 1.20 12.08 - Validation loss: 8.95 1.09 7.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 11.42 - MSE loss: 1.05 - Contrastive loss: 10.53: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.26it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 11.49 1.09 10.56 - Validation loss: 8.61 1.08 7.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 9.18 - MSE loss: 1.15 - Contrastive loss: 8.20: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.34it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training loss: 9.50 1.13 8.53 - Validation loss: 7.95 1.06 7.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 8.49 - MSE loss: 1.03 - Contrastive loss: 7.63: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.29it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 8.72 1.08 7.81 - Validation loss: 7.43 1.06 6.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 6.09 - MSE loss: 1.27 - Contrastive loss: 4.96: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.36it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training loss: 7.09 1.17 6.08 - Validation loss: 7.16 1.03 6.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 6.15 - MSE loss: 1.00 - Contrastive loss: 5.31: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.34it/s]\n",
      "  0%|                                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 6.81 1.06 5.92 - Validation loss: 7.13 1.03 6.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 7.13 - MSE loss: 0.94 - Contrastive loss: 6.39: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training loss: 7.08 1.04 6.25 - Validation loss: 6.32 1.02 5.49\n",
      "Best Epoch 9 - Best Validation loss: 6.319032669067383\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "max_epoch = 10\n",
    "best_loss = 1e10\n",
    "best_epoch = 0\n",
    "device = \"cuda\"\n",
    "loss_fn = nn.MSELoss()\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "while i < max_epoch:\n",
    "    train_loss = { \"loss\": [], \"loss_mse\": [], \"loss_con\": []}\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    \n",
    "    for IDs in progress_bar:\n",
    "        model.train()\n",
    "        X = torch.tensor(data.feature_df.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.float()\n",
    "        X = X.reshape(-1, max_len, X.shape[-1])\n",
    "        # X = X[:, :, -1:]\n",
    "        \n",
    "        pred, s = model(X)  # (batch_size, padded_length, feat_dim)\n",
    "        \n",
    "        loss_mse = loss_fn(pred, X) \n",
    "\n",
    "        loss_con = demo_contrastive_loss(s, X.shape[0])\n",
    "\n",
    "        loss = 1/(model.w1.pow(2)) * loss_mse + 1/(model.w2.pow(2)) * loss_con + torch.log(model.w1) + torch.log(model.w2)\n",
    "  \n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "        optimizer.step()\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        progress_bar.set_description(\"Epoch {0} - Training loss: {1:.2f} - MSE loss: {2:.2f} - Contrastive loss: {3:.2f}\".format(i, \n",
    "                loss.cpu().detach().numpy().item(), loss_mse.cpu().detach().numpy().item(), loss_con.cpu().detach().numpy().item())) \n",
    "        train_loss[\"loss\"].append(loss)\n",
    "        train_loss[\"loss_mse\"].append(loss_mse)\n",
    "        train_loss[\"loss_con\"].append(loss_con)\n",
    "    \n",
    "            \n",
    "    with torch.no_grad():\n",
    "        val_loss = { \"loss\": [], \"loss_mse\": [], \"loss_con\": []}\n",
    "        for IDs in val_dataloader:\n",
    "            model.eval()\n",
    "            X = torch.tensor(data.feature_df.loc[IDs].to_numpy()).to(device)\n",
    "            X = X.float()\n",
    "            X = X.reshape(-1, max_len, X.shape[-1])\n",
    "            # X = X[:, :, -1:]\n",
    "\n",
    "\n",
    "            pred, s = model(X)  # (batch_size, padded_length, feat_dim)\n",
    "        \n",
    "            loss_mse = loss_fn(pred, X) \n",
    "\n",
    "            loss_con = demo_contrastive_loss(s, X.shape[0])\n",
    "\n",
    "            loss = 1/(model.w1.pow(2)) * loss_mse + 1/(model.w2.pow(2)) * loss_con + torch.log(model.w1) + torch.log(model.w2)\n",
    "\n",
    "            val_loss[\"loss\"].append(loss)\n",
    "            val_loss[\"loss_mse\"].append(loss_mse)\n",
    "            val_loss[\"loss_con\"].append(loss_con)\n",
    "\n",
    "        train_loss[\"loss\"] = torch.tensor(train_loss[\"loss\"]).mean()\n",
    "        train_loss[\"loss_mse\"] = torch.tensor(train_loss[\"loss_mse\"]).mean()\n",
    "        train_loss[\"loss_con\"] = torch.tensor(train_loss[\"loss_con\"]).mean()\n",
    "        val_loss[\"loss\"] = torch.tensor(val_loss[\"loss\"]).mean()\n",
    "        val_loss[\"loss_mse\"] = torch.tensor(val_loss[\"loss_mse\"]).mean()\n",
    "        val_loss[\"loss_con\"] = torch.tensor(val_loss[\"loss_con\"]).mean()\n",
    "\n",
    "        if val_loss[\"loss\"] < best_loss:\n",
    "            best_loss = val_loss[\"loss\"]\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = i\n",
    "    \n",
    "        progress_bar.write(\"Epoch {0} - Training loss: {1:.2f} {2:.2f} {3:.2f} - Validation loss: {4:.2f} {5:.2f} {6:.2f}\".format(i, \n",
    "            train_loss[\"loss\"].cpu().detach().numpy().item(), train_loss[\"loss_mse\"].cpu().detach().numpy().item(), train_loss[\"loss_con\"].cpu().detach().numpy().item(),\n",
    "            val_loss[\"loss\"].cpu().detach().numpy().item(), val_loss[\"loss_mse\"].cpu().detach().numpy().item(), val_loss[\"loss_con\"].cpu().detach().numpy().item()))\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "tqdm.write(\"Best Epoch {} - Best Validation loss: {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetune_model = copy.deepcopy(best_model)\n",
    "optimizer = torch.optim.AdamW(finetune_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 190.31: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44.17it/s]\n",
      "Epoch 1 - Training loss: 179.38: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 46.66it/s]\n",
      "Epoch 2 - Training loss: 199.37:   0%|                                                                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 197.49 - Validation loss: 266.50\n",
      "Epoch 1 - Training loss: 193.02 - Validation loss: 264.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 194.42: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 43.68it/s]\n",
      "Epoch 3 - Training loss: 252.29: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45.58it/s]\n",
      "Epoch 4 - Training loss: 195.14:   0%|                                                                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 196.89 - Validation loss: 262.29\n",
      "Epoch 3 - Training loss: 220.50 - Validation loss: 259.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 202.30: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45.81it/s]\n",
      "Epoch 5 - Training loss: 177.72: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 46.39it/s]\n",
      "Epoch 6 - Training loss: 202.85:   0%|                                                                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 198.72 - Validation loss: 257.59\n",
      "Epoch 5 - Training loss: 188.57 - Validation loss: 254.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 142.39: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 43.97it/s]\n",
      "Epoch 7 - Training loss: 278.66: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44.32it/s]\n",
      "Epoch 8 - Training loss: 187.06:   0%|                                                                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training loss: 172.62 - Validation loss: 251.80\n",
      "Epoch 7 - Training loss: 227.18 - Validation loss: 248.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 191.96: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 43.77it/s]\n",
      "Epoch 9 - Training loss: 203.59: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training loss: 189.51 - Validation loss: 244.24\n",
      "Epoch 9 - Training loss: 192.86 - Validation loss: 240.24\n",
      "Best Epoch 9 - Best Validation loss: 240.23963928222656\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "max_epoch = 10\n",
    "best_loss = 1e10\n",
    "best_finetune_model = copy.deepcopy(best_model)\n",
    "best_epoch = 0\n",
    "device = \"cuda\"\n",
    "finetune_model.to(device)\n",
    "while i < max_epoch:\n",
    "    train_loss = []\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    \n",
    "    for IDs in progress_bar:\n",
    "        finetune_model.train()\n",
    "        \n",
    "        X = torch.tensor(data.feature_df.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.reshape(-1, max_len, X.shape[-1])\n",
    "        targets = torch.tensor(data.labels_df.loc[IDs].to_numpy()).to(device)\n",
    "        targets = targets.reshape(targets.shape[0], out_size, -1)\n",
    "        \n",
    "        pred = finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(finetune_model.parameters(), max_norm=4.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_description(\"Epoch {} - Training loss: {:.2f}\".format(i, loss)) \n",
    "        train_loss.append(loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for IDs in val_dataloader:\n",
    "            finetune_model.eval()\n",
    "            X = torch.tensor(data.feature_df.loc[IDs].to_numpy()).to(device)\n",
    "            X = X.reshape(-1, max_len, X.shape[-1])\n",
    "            targets = torch.tensor(data.labels_df.loc[IDs].to_numpy()).to(device)\n",
    "            targets = targets.reshape(targets.shape[0], out_size, -1)\n",
    "\n",
    "            pred = finetune_model.predict(X.float())\n",
    "            pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "            \n",
    "            loss = loss_fn(pred, targets)\n",
    "            val_loss.append(loss)\n",
    "\n",
    "        train_loss = torch.tensor(train_loss).mean()\n",
    "        val_loss = torch.tensor(val_loss).mean()\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_finetune_model = copy.deepcopy(finetune_model)\n",
    "            best_epoch = i\n",
    "    \n",
    "    progress_bar.write(\"Epoch {} - Training loss: {:.2f} - Validation loss: {:.2f}\".format(i, train_loss, val_loss))\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "tqdm.write(\"Best Epoch {} - Best Validation loss: {}\".format(best_epoch, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE loss: 192.38543701171875\n",
      "Test RMSE loss: 13.870307922363281\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "with torch.no_grad():\n",
    "    for IDs in test_dataloader:\n",
    "        best_finetune_model.eval()\n",
    "        X = torch.tensor(data.feature_df.loc[IDs].to_numpy()).to(device)\n",
    "        X = X.reshape(-1, max_len, X.shape[-1])\n",
    "        targets = torch.tensor(data.labels_df.loc[IDs].to_numpy()).to(device)\n",
    "        targets = targets.reshape(targets.shape[0], out_size, -1)\n",
    "        \n",
    "\n",
    "        pred = best_finetune_model.predict(X.float())\n",
    "        pred = pred.reshape(X.shape[0], out_size, -1)\n",
    "        loss = loss_fn(pred, targets)\n",
    "\n",
    "\n",
    "        test_loss.append(loss)\n",
    "\n",
    "\n",
    "test_loss = torch.tensor(test_loss).mean()\n",
    "print(\"Test MSE loss: {}\".format(test_loss))\n",
    "print(\"Test RMSE loss: {}\".format(np.sqrt(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"../img/img_8.PNG\"/><center/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://arxiv.org/abs/2302.00861\n",
    "2. https://github.com/gzerveas/mvts_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Pretrain\n",
    "\n",
    "Test loss: 17.563016891479492\n",
    "\n",
    "Test loss: 24.951457977294922\n",
    "\n",
    "Test loss: 13.356301307678223\n",
    "\n",
    "Test loss: 15.543647766113281\n",
    "\n",
    "Test loss: 10.70577621459961\n",
    "\n",
    "Test loss: 11.653989791870117\n",
    "\n",
    "Test loss: 28.348876953125\n",
    "\n",
    "Test loss: 9.66756534576416\n",
    "\n",
    "Test loss: 22.540130615234375\n",
    "\n",
    "Test loss: 20.3616943359375\n",
    "\n",
    "Pretrain \n",
    "\n",
    "Test loss: 11.152569770812988\n",
    "\n",
    "Test loss: 8.513360023498535\n",
    "\n",
    "Test loss: 20.0480899810791\n",
    "\n",
    "Test loss: 12.694931983947754\n",
    "\n",
    "Test loss: 15.458633422851562\n",
    "\n",
    "Test loss: 16.706832885742188\n",
    "\n",
    "Test loss: 11.432455062866211\n",
    "\n",
    "Test loss: 7.263433456420898\n",
    "\n",
    "Test loss: 15.908415794372559\n",
    "\n",
    "Test loss: 17.736656188964844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
